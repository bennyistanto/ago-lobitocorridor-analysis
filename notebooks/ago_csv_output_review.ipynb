{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b496080-ec59-42ff-8a68-dfbf3a34855b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== File presence check (project outputs + /mnt/data fallback) ===\n",
      "iso          -> FOUND | /mnt/c/Users/benny/OneDrive/Documents/Github/ago-lobitocorridor-analysis/outputs/tables/huambo_kpis_isochrones.csv\n",
      "risk         -> FOUND | /mnt/c/Users/benny/OneDrive/Documents/Github/ago-lobitocorridor-analysis/outputs/tables/huambo_roads_flood_risk_summary.csv\n",
      "muni         -> FOUND | /mnt/c/Users/benny/OneDrive/Documents/Github/ago-lobitocorridor-analysis/outputs/tables/huambo_municipality_indicators.csv\n",
      "corr         -> FOUND | /mnt/c/Users/benny/OneDrive/Documents/Github/ago-lobitocorridor-analysis/outputs/tables/huambo_corr_with_rural_poverty.csv\n",
      "prof         -> FOUND | /mnt/c/Users/benny/OneDrive/Documents/Github/ago-lobitocorridor-analysis/outputs/tables/huambo_municipality_profiles.csv\n",
      "rank         -> FOUND | /mnt/c/Users/benny/OneDrive/Documents/Github/ago-lobitocorridor-analysis/outputs/tables/huambo_priority_muni_rank.csv\n",
      "scn_meta     -> FOUND | /mnt/c/Users/benny/OneDrive/Documents/Github/ago-lobitocorridor-analysis/outputs/tables/huambo_priority_scenarios.meta.json\n",
      "scn_sum      -> FOUND | /mnt/c/Users/benny/OneDrive/Documents/Github/ago-lobitocorridor-analysis/outputs/tables/huambo_priority_scenarios_summary.csv\n",
      "site         -> FOUND | /mnt/c/Users/benny/OneDrive/Documents/Github/ago-lobitocorridor-analysis/outputs/tables/huambo_site_audit_points.csv\n",
      "proj         -> FOUND | /mnt/c/Users/benny/OneDrive/Documents/Github/ago-lobitocorridor-analysis/outputs/tables/huambo_project_kpis.csv\n",
      "lookup       -> FOUND | /mnt/c/Users/benny/OneDrive/Documents/Github/ago-lobitocorridor-analysis/outputs/tables/huambo_admin2_lookup.csv\n",
      "admin2_rank  -> FOUND | /mnt/c/Users/benny/OneDrive/Documents/Github/ago-lobitocorridor-analysis/outputs/tables/huambo_priority_admin2_rank.csv\n",
      "clusters     -> FOUND | /mnt/c/Users/benny/OneDrive/Documents/Github/ago-lobitocorridor-analysis/outputs/tables/huambo_priority_clusters.csv\n",
      "catch_kpi    -> FOUND | /mnt/c/Users/benny/OneDrive/Documents/Github/ago-lobitocorridor-analysis/outputs/tables/huambo_catchments_kpis.csv\n",
      "clust_syn    -> FOUND | /mnt/c/Users/benny/OneDrive/Documents/Github/ago-lobitocorridor-analysis/outputs/tables/huambo_cluster_synergies.csv\n",
      "site_syn     -> FOUND | /mnt/c/Users/benny/OneDrive/Documents/Github/ago-lobitocorridor-analysis/outputs/tables/huambo_site_synergies.csv\n",
      "od_grav      -> FOUND | /mnt/c/Users/benny/OneDrive/Documents/Github/ago-lobitocorridor-analysis/outputs/tables/huambo_od_gravity.csv\n",
      "od_zone      -> FOUND | /mnt/c/Users/benny/OneDrive/Documents/Github/ago-lobitocorridor-analysis/outputs/tables/huambo_od_zone_attrs.csv\n",
      "od_agents    -> FOUND | /mnt/c/Users/benny/OneDrive/Documents/Github/ago-lobitocorridor-analysis/outputs/tables/huambo_od_agents.csv\n",
      "\n",
      "[Isochrones] PASS shape=(4, 15)\n",
      "  monotonic pop/cells: PASS\n",
      "   aoi  travel_cut_min  pop_within  cells_within  area_km2_within   pop_pct  cropland_km2  cropland_pct  electrified_cells  electrified_pct  cell_area_km2  note_electrification_denominator_zero  pop_total  cropland_total_km2  electrified_total_cells\n",
      "huambo            30.0 1301172.250        2230.0           2230.0 43.412978    879.630005      8.660409              414.0        70.408163            1.0                                  False  2997196.5        10156.910156                    588.0\n",
      "huambo            60.0 1635859.875        7215.0           7215.0 54.579667   2212.280029     21.781034              448.0        76.190476            1.0                                  False  2997196.5        10156.910156                    588.0\n",
      "huambo           120.0 2438910.000       21964.0          21964.0 81.373043   5996.369629     59.037340              563.0        95.748299            1.0                                  False  2997196.5        10156.910156                    588.0\n",
      "[Roads×Flood] ranges: PASS\n",
      "  total_road_cells: 12523\n",
      "  total_risk_cells: 727\n",
      "  risk_near_priority_cells: 67\n",
      "  method: Risk=roads ∩ flood; near-priority=within 1 cell of Top10% priority; method=fraction≥0.25; roads_all_touched=False\n",
      "  fraction_min: 0.25\n",
      "[Muni indicators] PASS rows=11 unique_adm2=11\n",
      "  mean numeric NA share: 0.00%\n",
      "  sample cols: ADM2CD_c, NAM_2, NAM_1, communications__telephone, communications__internet, communications__newspaper, communications__radio, communications__television, communications__none, foodinsecurity__went_without_food ...\n",
      "[Muni correlations] n<=#ADM2: PASS n_max=11 #ADM2=11\n",
      "         theme           var         r        p  n\n",
      "       poverty poverty_total  0.956362 0.000004 11\n",
      "       outflow         other -0.926525 0.000042 11\n",
      "       poverty poverty_urban  0.804406 0.002833 11\n",
      "communications    television -0.655404 0.028583 11\n",
      "communications     newspaper -0.584704 0.058852 11\n",
      "[Profiles] quintile present: PASS rows=11\n",
      "  quintile counts:\n",
      " poverty_quintile\n",
      "1    3\n",
      "2    2\n",
      "5    2\n",
      "3    2\n",
      "4    2\n",
      "Name: count, dtype: int64\n",
      "[Priority rank] columns: FAIL shape=(11, 17)\n",
      "[Scenarios] ids match meta: PASS count=4\n",
      "  overlap_pct_vs_baseline: min=0.05 mean=0.10 max=0.18\n",
      "  jaccard_vs_baseline: min=0.05 mean=0.10 max=0.17\n",
      "  selected_cells: min=307.00 mean=775.75 max=1200.00\n",
      "  selected_km2: min=257.63 mean=651.32 max=1007.70\n",
      "[Site audit points] has XY: PASS shape=(39, 16)\n",
      "[Project KPIs] shape: (39, 33)\n",
      "[Admin2 Lookup] columns: PASS shape=(11, 4)\n",
      "  unique ADM2CD_c: PASS\n",
      "  sequential lab: PASS\n",
      "  provinces: Huambo\n",
      "  municipalities: 11\n",
      " lab  ADM2CD_c  NAM_1    NAM_2\n",
      "   1 AGO009001 Huambo Bailundo\n",
      "   2 AGO009002 Huambo    Caala\n",
      "   3 AGO009003 Huambo   Ekunha\n",
      "[Priority Admin2 Rank] columns: PASS shape=(11, 13)\n",
      "  contiguous ranks: PASS\n",
      "  rank range: 1 to 11\n",
      "  score range [0-1]: PASS [0.4822, 0.7085]\n",
      "  selected municipalities: 2/11 (18.2%)\n",
      "  share_selected range [0-1]: PASS [0.0000, 0.0369]\n",
      "  Top 5 priority municipalities:\n",
      "     NAM_2    score  rank  selected\n",
      "    Ekunha 0.708481     1     False\n",
      "    Huambo 0.703294     2      True\n",
      "     Caala 0.665319     3      True\n",
      "  Bailundo 0.618705     4     False\n",
      "Katchiungo 0.607983     5     False\n",
      "[Priority clusters] present: PASS shape=(2, 20)\n",
      "  sample cols: cluster_id\n",
      " cluster_id\n",
      "          1\n",
      "          2\n",
      "[Catchments KPIs] columns: PASS shape=(156, 10)\n",
      "  monotone area by site: PASS | 100% sites OK\n",
      "[Cluster synergies] present: PASS shape=(2, 15)\n",
      "  columns: cluster_id, lon, lat, dist_km_nearest_gov, dist_km_nearest_wb, dist_km_nearest_oth, count_gov_le5km, count_wb_le5km, count_oth_le5km, count_gov_le10km, count_wb_le10km, count_oth_le10km ...\n",
      "[Site synergies] present: PASS shape=(39, 15)\n",
      "  columns: site_id, lon, lat, dist_km_nearest_gov, dist_km_nearest_wb, dist_km_nearest_oth, count_gov_le5km, count_wb_le5km, count_oth_le5km, count_gov_le10km, count_wb_le10km, count_oth_le10km ...\n",
      "[OD zone attrs] has lon/lat: PASS | has zone id: PASS shape=(11, 8)\n",
      "[OD gravity] columns: PASS rows=121\n",
      "  non-negative flows: PASS\n",
      "  total trips=1,000,000 | flow-weighted mean dist=41.1 km\n",
      "  mean asymmetry |F - F^T| = 0.00\n",
      "[OD agents] columns: PASS N=121\n",
      "  o_lon in [-180,180]: 100.00%\n",
      "  d_lon in [-180,180]: 100.00%\n",
      "  o_lat in [-90,90]: 100.00%\n",
      "  d_lat in [-90,90]: 100.00%\n",
      "\n",
      "================================================================================\n",
      "CROSS-FILE VALIDATION\n",
      "================================================================================\n",
      "[Lookup ↔ Admin2 Rank] ADM2CD_c match: PASS\n",
      "[Lookup ↔ Muni Indicators] count match: PASS lookup=11 muni_unique=11\n",
      "[OD zones ↔ Lookup] zone count matches: PASS zones=11 lookup=11\n",
      "\n",
      "================================================================================\n",
      "VALIDATION COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json, pandas as pd, numpy as np\n",
    "\n",
    "# ---------- Config ----------\n",
    "AOI = \"huambo\"  # change if needed\n",
    "# Primary project outputs path\n",
    "B1 = Path(\"/mnt/c/Users/benny/OneDrive/Documents/Github/ago-lobitocorridor-analysis/outputs/tables\")\n",
    "# Fallback for ad-hoc uploads provided in this session\n",
    "B2 = Path(\"/mnt/d/temp/wbg/iso3/ago/lobitocorridor/outputs/tables\")\n",
    "\n",
    "SEARCH_DIRS = [B1, B2]\n",
    "\n",
    "def resolve(fname: str) -> Path:\n",
    "    for base in SEARCH_DIRS:\n",
    "        p = base / fname\n",
    "        if p.exists():\n",
    "            return p\n",
    "    # Not found anywhere -> return default in B1 so messages still show path\n",
    "    return B1 / fname\n",
    "\n",
    "def ok(x): return \"PASS\" if x else \"FAIL\"\n",
    "def exists_nonempty(p: Path) -> bool:\n",
    "    try:\n",
    "        return p.exists() and p.stat().st_size > 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# ---------- Known / expected filenames ----------\n",
    "paths = {\n",
    "    # Step 00–10 (yours, unchanged)\n",
    "    \"iso\":        resolve(f\"{AOI}_kpis_isochrones.csv\"),\n",
    "    \"risk\":       resolve(f\"{AOI}_roads_flood_risk_summary.csv\"),\n",
    "    \"muni\":       resolve(f\"{AOI}_municipality_indicators.csv\"),\n",
    "    \"corr\":       resolve(f\"{AOI}_corr_with_rural_poverty.csv\"),\n",
    "    \"prof\":       resolve(f\"{AOI}_municipality_profiles.csv\"),\n",
    "    \"rank\":       resolve(f\"{AOI}_priority_muni_rank.csv\"),\n",
    "    \"scn_meta\":   resolve(f\"{AOI}_priority_scenarios.meta.json\"),\n",
    "    \"scn_sum\":    resolve(f\"{AOI}_priority_scenarios_summary.csv\"),\n",
    "    \"site\":       resolve(f\"{AOI}_site_audit_points.csv\"),\n",
    "    \"proj\":       resolve(f\"{AOI}_project_kpis.csv\"),\n",
    "    \"lookup\":     resolve(f\"{AOI}_admin2_lookup.csv\"),\n",
    "    \"admin2_rank\":resolve(f\"{AOI}_priority_admin2_rank.csv\"),\n",
    "    \"clusters\":   resolve(f\"{AOI}_priority_clusters.csv\"),\n",
    "    \"catch_kpi\":  resolve(f\"{AOI}_catchments_kpis.csv\"),\n",
    "    \"clust_syn\":  resolve(f\"{AOI}_cluster_synergies.csv\"),\n",
    "    \"site_syn\":   resolve(f\"{AOI}_site_synergies.csv\"),\n",
    "    \"od_grav\":    resolve(f\"{AOI}_od_gravity.csv\"),\n",
    "    \"od_zone\":    resolve(f\"{AOI}_od_zone_attrs.csv\"),\n",
    "    \"od_agents\":  resolve(f\"{AOI}_od_agents.csv\"),\n",
    "}\n",
    "\n",
    "print(\"=== File presence check (project outputs + /mnt/data fallback) ===\")\n",
    "for k, p in paths.items():\n",
    "    print(f\"{k:12s} ->\", \"FOUND\" if p.exists() else \"MISSING\", f\"| {p}\")\n",
    "print()\n",
    "\n",
    "# ------------------------------\n",
    "# 1) Isochrones (Step 05/08)\n",
    "# ------------------------------\n",
    "try:\n",
    "    p = paths[\"iso\"]\n",
    "    if exists_nonempty(p):\n",
    "        iso = pd.read_csv(p)\n",
    "        exp = {\"aoi\",\"travel_cut_min\",\"pop_within\",\"cells_within\",\"area_km2_within\"}\n",
    "        print(\"[Isochrones]\", ok(exp.issubset(iso.columns)), f\"shape={iso.shape}\")\n",
    "        iso_s = iso.sort_values(\"travel_cut_min\")\n",
    "        mono_pop   = (iso_s[\"pop_within\"].diff().fillna(0)  >= -1e-6).all()\n",
    "        mono_cells = (iso_s[\"cells_within\"].diff().fillna(0) >= -1e-6).all()\n",
    "        print(\"  monotonic pop/cells:\", ok(mono_pop and mono_cells))\n",
    "        print(iso.head(3).to_string(index=False))\n",
    "    else:\n",
    "        print(\"[Isochrones] MISSING or EMPTY\")\n",
    "except Exception as e:\n",
    "    print(\"[Isochrones] ERROR:\", e)\n",
    "\n",
    "# ------------------------------\n",
    "# 2) Roads × Flood\n",
    "# ------------------------------\n",
    "try:\n",
    "    p = paths[\"risk\"]\n",
    "    if exists_nonempty(p):\n",
    "        risk = pd.read_csv(p)\n",
    "        if not risk.empty:\n",
    "            r = risk.iloc[0]\n",
    "            def fnum(k):\n",
    "                try: return float(r.get(k, np.nan))\n",
    "                except: return np.nan\n",
    "            ok_range = (0 <= fnum(\"risk_pct_of_roads\") <= 100) and (0 <= fnum(\"near_prio_pct_of_risk\") <= 100)\n",
    "            print(\"[Roads×Flood] ranges:\", ok(ok_range))\n",
    "            for k in [\"total_road_cells\",\"total_risk_cells\",\"risk_near_priority_cells\"]:\n",
    "                print(f\"  {k}:\", r.get(k, \"<missing>\"))\n",
    "            print(\"  method:\", r.get(\"notes\",\"\"))\n",
    "            print(\"  fraction_min:\", r.get(\"flood_exceed_fraction_min\", np.nan))\n",
    "        else:\n",
    "            print(\"[Roads×Flood] EMPTY\")\n",
    "    else:\n",
    "        print(\"[Roads×Flood] MISSING or EMPTY\")\n",
    "except Exception as e:\n",
    "    print(\"[Roads×Flood] ERROR:\", e)\n",
    "\n",
    "# ------------------------------\n",
    "# 3) Municipality indicators\n",
    "# ------------------------------\n",
    "muni = None\n",
    "try:\n",
    "    p = paths[\"muni\"]\n",
    "    if exists_nonempty(p):\n",
    "        muni = pd.read_csv(p)\n",
    "        idc = [c for c in [\"ADM2CD_c\",\"NAM_1\",\"NAM_2\"] if c in muni.columns]\n",
    "        n_unique = len(muni[idc].drop_duplicates()) if idc else None\n",
    "        print(\"[Muni indicators]\", ok(n_unique==len(muni)),\n",
    "              f\"rows={len(muni)} unique_adm2={n_unique}\")\n",
    "        num = muni.select_dtypes(include=\"number\")\n",
    "        na_mean = float(num.isna().mean().mean()) if not num.empty else np.nan\n",
    "        print(\"  mean numeric NA share:\", f\"{na_mean:.2%}\")\n",
    "        print(\"  sample cols:\", \", \".join(list(muni.columns)[:10]), \"...\")\n",
    "    else:\n",
    "        print(\"[Muni indicators] MISSING or EMPTY\")\n",
    "except Exception as e:\n",
    "    print(\"[Muni indicators] ERROR:\", e)\n",
    "\n",
    "# ------------------------------\n",
    "# 4) Correlations\n",
    "# ------------------------------\n",
    "try:\n",
    "    p = paths[\"corr\"]\n",
    "    if exists_nonempty(p):\n",
    "        corr = pd.read_csv(p)\n",
    "        if not corr.empty and {\"theme\",\"var\",\"r\",\"p\",\"n\"}.issubset(corr.columns):\n",
    "            n_max = int(pd.to_numeric(corr[\"n\"], errors=\"coerce\").max())\n",
    "            n_rows = len(muni) if muni is not None else None\n",
    "            print(\"[Muni correlations] n<=#ADM2:\", ok(n_rows is None or n_max <= n_rows),\n",
    "                  f\"n_max={n_max} #ADM2={n_rows}\")\n",
    "            top = corr.assign(absr=np.abs(pd.to_numeric(corr[\"r\"], errors=\"coerce\")))\\\n",
    "                      .sort_values(\"absr\", ascending=False).head(5)\n",
    "            print(top[[\"theme\",\"var\",\"r\",\"p\",\"n\"]].to_string(index=False))\n",
    "        else:\n",
    "            print(\"[Muni correlations] present but missing cols or EMPTY\")\n",
    "    else:\n",
    "        print(\"[Muni correlations] MISSING or EMPTY\")\n",
    "except Exception as e:\n",
    "    print(\"[Muni correlations] ERROR:\", e)\n",
    "\n",
    "# ------------------------------\n",
    "# 5) Profiles\n",
    "# ------------------------------\n",
    "try:\n",
    "    p = paths[\"prof\"]\n",
    "    if exists_nonempty(p):\n",
    "        prof = pd.read_csv(p)\n",
    "        has_q = \"poverty_quintile\" in prof.columns\n",
    "        print(\"[Profiles] quintile present:\", ok(has_q), f\"rows={len(prof)}\")\n",
    "        if has_q:\n",
    "            print(\"  quintile counts:\\n\", prof[\"poverty_quintile\"].value_counts(dropna=False))\n",
    "    else:\n",
    "        print(\"[Profiles] MISSING or EMPTY\")\n",
    "except Exception as e:\n",
    "    print(\"[Profiles] ERROR:\", e)\n",
    "\n",
    "# ------------------------------\n",
    "# 6) Priority rank (legacy)\n",
    "# ------------------------------\n",
    "try:\n",
    "    p = paths[\"rank\"]\n",
    "    if exists_nonempty(p):\n",
    "        rank = pd.read_csv(p)\n",
    "        need = {\"ADM2CD_c\",\"NAM_1\",\"NAM_2\",\"score\",\"rank\",\"selected\"}\n",
    "        print(\"[Priority rank] columns:\", ok(need.issubset(rank.columns)), f\"shape={rank.shape}\")\n",
    "        if \"rank\" in rank.columns and rank[\"rank\"].notna().any():\n",
    "            rseq = sorted(rank[\"rank\"].dropna().astype(int))\n",
    "            contig = (rseq == list(range(min(rseq), max(rseq)+1)))\n",
    "            print(\"  contiguous ranks:\", ok(contig))\n",
    "    else:\n",
    "        print(\"[Priority rank] MISSING or EMPTY\")\n",
    "except Exception as e:\n",
    "    print(\"[Priority rank] ERROR:\", e)\n",
    "\n",
    "# ------------------------------\n",
    "# 7) Scenarios meta/summary\n",
    "# ------------------------------\n",
    "try:\n",
    "    scn_ids_meta = []\n",
    "    pm = paths[\"scn_meta\"]\n",
    "    if exists_nonempty(pm):\n",
    "        meta = json.loads(pm.read_text())\n",
    "        if isinstance(meta, list):\n",
    "            scn_ids_meta = [d.get(\"id\") for d in meta if isinstance(d, dict)]\n",
    "    ps = paths[\"scn_sum\"]\n",
    "    if exists_nonempty(ps):\n",
    "        scn = pd.read_csv(ps)\n",
    "        scn_ids_sum = sorted(scn[\"scenario_id\"].unique()) if \"scenario_id\" in scn else []\n",
    "        scn_ok = not scn_ids_meta or (set(scn_ids_meta) == set(scn_ids_sum))\n",
    "        print(\"[Scenarios] ids match meta:\", ok(scn_ok), f\"count={len(scn_ids_sum)}\")\n",
    "        for k in [\"overlap_pct_vs_baseline\",\"jaccard_vs_baseline\",\"selected_cells\",\"selected_km2\"]:\n",
    "            if k in scn:\n",
    "                print(f\"  {k}: min={scn[k].min():.2f} mean={scn[k].mean():.2f} max={scn[k].max():.2f}\")\n",
    "    else:\n",
    "        print(\"[Scenarios] MISSING or EMPTY\")\n",
    "except Exception as e:\n",
    "    print(\"[Scenarios] ERROR:\", e)\n",
    "\n",
    "# ------------------------------\n",
    "# 8) Site audit points\n",
    "# ------------------------------\n",
    "try:\n",
    "    p = paths[\"site\"]\n",
    "    if exists_nonempty(p):\n",
    "        site = pd.read_csv(p)\n",
    "        cols = {c.lower() for c in site.columns}\n",
    "        has_xy = any(c in cols for c in [\"x\",\"lon\",\"longitude\"]) and any(c in cols for c in [\"y\",\"lat\",\"latitude\"])\n",
    "        print(\"[Site audit points] has XY:\", ok(has_xy), f\"shape={site.shape}\")\n",
    "    else:\n",
    "        print(\"[Site audit points] MISSING or EMPTY\")\n",
    "except Exception as e:\n",
    "    print(\"[Site audit points] ERROR:\", e)\n",
    "\n",
    "# ------------------------------\n",
    "# 9) Project KPIs\n",
    "# ------------------------------\n",
    "try:\n",
    "    p = paths[\"proj\"]\n",
    "    if exists_nonempty(p):\n",
    "        proj = pd.read_csv(p)\n",
    "        print(\"[Project KPIs] shape:\", proj.shape)\n",
    "    else:\n",
    "        print(\"[Project KPIs] MISSING or EMPTY\")\n",
    "except Exception as e:\n",
    "    print(\"[Project KPIs] ERROR:\", e)\n",
    "\n",
    "# ------------------------------\n",
    "# 10) Admin2 Lookup\n",
    "# ------------------------------\n",
    "lookup = None\n",
    "try:\n",
    "    p = paths[\"lookup\"]\n",
    "    if exists_nonempty(p):\n",
    "        lookup = pd.read_csv(p)\n",
    "        need_cols = {\"lab\", \"ADM2CD_c\", \"NAM_1\", \"NAM_2\"}\n",
    "        has_cols = need_cols.issubset(lookup.columns)\n",
    "        print(\"[Admin2 Lookup] columns:\", ok(has_cols), f\"shape={lookup.shape}\")\n",
    "        if has_cols:\n",
    "            is_unique = lookup[\"ADM2CD_c\"].is_unique\n",
    "            print(\"  unique ADM2CD_c:\", ok(is_unique))\n",
    "            if \"lab\" in lookup.columns:\n",
    "                expected_labs = list(range(1, len(lookup) + 1))\n",
    "                actual_labs = sorted(lookup[\"lab\"].tolist())\n",
    "                labs_sequential = (actual_labs == expected_labs)\n",
    "                print(\"  sequential lab:\", ok(labs_sequential))\n",
    "            if \"NAM_1\" in lookup.columns:\n",
    "                provinces = lookup[\"NAM_1\"].unique()\n",
    "                print(f\"  provinces: {', '.join(provinces)}\")\n",
    "            if \"NAM_2\" in lookup.columns:\n",
    "                n_municipalities = lookup[\"NAM_2\"].nunique()\n",
    "                print(f\"  municipalities: {n_municipalities}\")\n",
    "            print(lookup.head(3).to_string(index=False))\n",
    "    else:\n",
    "        print(\"[Admin2 Lookup] MISSING or EMPTY\")\n",
    "except Exception as e:\n",
    "    print(\"[Admin2 Lookup] ERROR:\", e)\n",
    "\n",
    "# ------------------------------\n",
    "# 11) Priority Admin2 Rank\n",
    "# ------------------------------\n",
    "admin2_rank = None\n",
    "try:\n",
    "    p = paths[\"admin2_rank\"]\n",
    "    if exists_nonempty(p):\n",
    "        admin2_rank = pd.read_csv(p)\n",
    "        need_cols = {\"ADM2CD_c\", \"NAM_1\", \"NAM_2\", \"score\", \"rank\", \"selected\", \"share_selected\"}\n",
    "        has_cols = need_cols.issubset(admin2_rank.columns)\n",
    "        print(\"[Priority Admin2 Rank] columns:\", ok(has_cols), f\"shape={admin2_rank.shape}\")\n",
    "        if has_cols:\n",
    "            if \"rank\" in admin2_rank.columns and admin2_rank[\"rank\"].notna().any():\n",
    "                rseq = sorted(admin2_rank[\"rank\"].dropna().astype(int))\n",
    "                contig = (rseq == list(range(min(rseq), max(rseq)+1)))\n",
    "                print(\"  contiguous ranks:\", ok(contig))\n",
    "                print(f\"  rank range: {min(rseq)} to {max(rseq)}\")\n",
    "            if \"score\" in admin2_rank.columns:\n",
    "                score_min = admin2_rank[\"score\"].min()\n",
    "                score_max = admin2_rank[\"score\"].max()\n",
    "                score_range_ok = (0 <= score_min <= 1) and (0 <= score_max <= 1)\n",
    "                print(\"  score range [0-1]:\", ok(score_range_ok), f\"[{score_min:.4f}, {score_max:.4f}]\")\n",
    "            if \"selected\" in admin2_rank.columns:\n",
    "                n_selected = admin2_rank[\"selected\"].sum()\n",
    "                pct_selected = (n_selected / len(admin2_rank)) * 100\n",
    "                print(f\"  selected municipalities: {n_selected}/{len(admin2_rank)} ({pct_selected:.1f}%)\")\n",
    "            if \"share_selected\" in admin2_rank.columns:\n",
    "                share_min = admin2_rank[\"share_selected\"].min()\n",
    "                share_max = admin2_rank[\"share_selected\"].max()\n",
    "                share_range_ok = (0 <= share_min <= 1) and (0 <= share_max <= 1)\n",
    "                print(\"  share_selected range [0-1]:\", ok(share_range_ok), f\"[{share_min:.4f}, {share_max:.4f}]\")\n",
    "            if \"rank\" in admin2_rank.columns:\n",
    "                top5 = admin2_rank.sort_values(\"rank\").head(5)\n",
    "                print(\"  Top 5 priority municipalities:\")\n",
    "                print(top5[[\"NAM_2\", \"score\", \"rank\", \"selected\"]].to_string(index=False))\n",
    "    else:\n",
    "        print(\"[Priority Admin2 Rank] MISSING or EMPTY\")\n",
    "except Exception as e:\n",
    "    print(\"[Priority Admin2 Rank] ERROR:\", e)\n",
    "\n",
    "# ------------------------------\n",
    "# 12) Priority clusters (schema-agnostic summary)\n",
    "# ------------------------------\n",
    "try:\n",
    "    p = paths[\"clusters\"]\n",
    "    if exists_nonempty(p):\n",
    "        cl = pd.read_csv(p)\n",
    "        print(\"[Priority clusters] present:\", ok(True), f\"shape={cl.shape}\")\n",
    "        # Try common fields if available\n",
    "        common = [c for c in [\"cluster_id\",\"cells\",\"km2\",\"score_mean\",\"selected\"] if c in cl.columns]\n",
    "        if common:\n",
    "            print(\"  sample cols:\", \", \".join(common))\n",
    "            print(cl[common].head(5).to_string(index=False))\n",
    "        else:\n",
    "            print(\"  columns:\", \", \".join(cl.columns[:12]), \"...\")\n",
    "    else:\n",
    "        print(\"[Priority clusters] MISSING or EMPTY\")\n",
    "except Exception as e:\n",
    "    print(\"[Priority clusters] ERROR:\", e)\n",
    "\n",
    "# ------------------------------\n",
    "# 13) Catchments KPIs\n",
    "# ------------------------------\n",
    "try:\n",
    "    if exists_nonempty(paths[\"catch_kpi\"]):\n",
    "        ck = pd.read_csv(paths[\"catch_kpi\"])\n",
    "        need = {\"site_index\",\"thresh_min\"}\n",
    "        print(\"[Catchments KPIs] columns:\", ok(need.issubset(ck.columns)), f\"shape={ck.shape}\")\n",
    "\n",
    "        # Types & ordering (robust to writer changes)\n",
    "        ck[\"thresh_min\"] = pd.to_numeric(ck[\"thresh_min\"], errors=\"coerce\")\n",
    "        if \"area_km2\" in ck.columns:\n",
    "            ck[\"area_km2\"] = pd.to_numeric(ck[\"area_km2\"], errors=\"coerce\")\n",
    "\n",
    "            ck_sorted = ck.sort_values([\"site_index\",\"thresh_min\"])\n",
    "\n",
    "            # Vectorized monotone check (no ambiguous truth values, no deprecation)\n",
    "            mono_series = (\n",
    "                ck_sorted\n",
    "                .groupby(\"site_index\", group_keys=False)[\"area_km2\"]\n",
    "                .apply(lambda s: (s.diff().fillna(0) >= -1e-6).all())\n",
    "            )\n",
    "            mono_pct = 100.0 * float(mono_series.mean()) if len(mono_series) else float(\"nan\")\n",
    "            print(f\"  monotone area by site: {ok(bool(mono_series.all()))} | {mono_pct:.0f}% sites OK\")\n",
    "        else:\n",
    "            print(\"  area_km2 missing → skip monotonicity\")\n",
    "    else:\n",
    "        print(\"[Catchments KPIs] MISSING or EMPTY\")\n",
    "except Exception as e:\n",
    "    print(\"[Catchments KPIs] ERROR:\", e)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 14) Synergies (clusters & sites)\n",
    "# ------------------------------\n",
    "def _summarize_synergy(name, pth):\n",
    "    try:\n",
    "        if exists_nonempty(pth):\n",
    "            df = pd.read_csv(pth)\n",
    "            print(f\"[{name}] present:\", ok(True), f\"shape={df.shape}\")\n",
    "            # Try some common helpful summaries if columns exist\n",
    "            maybe_cols = set(df.columns.str.lower())\n",
    "            if {\"site_index\",\"cluster_id\"}.issubset(maybe_cols):\n",
    "                # group by cluster → count sites\n",
    "                si = [c for c in df.columns if c.lower()==\"site_index\"][0]\n",
    "                ci = [c for c in df.columns if c.lower()==\"cluster_id\"][0]\n",
    "                g = df.groupby(ci)[si].nunique().describe()[[\"count\",\"mean\",\"max\"]]\n",
    "                print(f\"  sites per cluster (count/mean/max): {g.to_dict()}\")\n",
    "            if {\"km2\",\"beneficiaries\"}.issubset(maybe_cols):\n",
    "                k2 = [c for c in df.columns if c.lower()==\"km2\"][0]\n",
    "                ben = [c for c in df.columns if c.lower()==\"beneficiaries\"][0]\n",
    "                print(f\"  totals → km2={df[k2].sum():.1f}, beneficiaries={int(df[ben].sum()):,}\")\n",
    "            print(\"  columns:\", \", \".join(df.columns[:12]), \"...\")\n",
    "        else:\n",
    "            print(f\"[{name}] MISSING or EMPTY\")\n",
    "    except Exception as e:\n",
    "        print(f\"[{name}] ERROR:\", e)\n",
    "\n",
    "_summarize_synergy(\"Cluster synergies\", paths[\"clust_syn\"])\n",
    "_summarize_synergy(\"Site synergies\", paths[\"site_syn\"])\n",
    "\n",
    "# ------------------------------\n",
    "# 15) OD-Lite\n",
    "# ------------------------------\n",
    "try:\n",
    "    # Zone attributes\n",
    "    pz = paths[\"od_zone\"]\n",
    "    grav = paths[\"od_grav\"]\n",
    "    ag = paths[\"od_agents\"]\n",
    "\n",
    "    if exists_nonempty(pz):\n",
    "        Z = pd.read_csv(pz)\n",
    "        has_xy = {\"lon\",\"lat\"}.issubset(Z.columns)\n",
    "        has_id = any(c in Z.columns for c in [\"ADM2CD_c\",\"adm2cd_c\",\"id\",\"lab\"])\n",
    "        print(\"[OD zone attrs] has lon/lat:\", ok(has_xy), \"| has zone id:\", ok(has_id), f\"shape={Z.shape}\")\n",
    "    else:\n",
    "        Z = None\n",
    "        print(\"[OD zone attrs] MISSING or EMPTY\")\n",
    "\n",
    "    # Gravity table\n",
    "    if exists_nonempty(grav):\n",
    "        G = pd.read_csv(grav)\n",
    "        need = {\"oi\",\"dj\",\"flow\",\"dist_km\"}\n",
    "        print(\"[OD gravity] columns:\", ok(need.issubset(G.columns)), f\"rows={len(G)}\")\n",
    "        if need.issubset(G.columns):\n",
    "            # Basic validity: non-negative flows, diagonal allowed but distance should be ~0 there\n",
    "            nonneg = (G[\"flow\"] >= -1e-9).all()\n",
    "            print(\"  non-negative flows:\", ok(nonneg))\n",
    "            # Compute a couple of quick stats\n",
    "            total = G[\"flow\"].sum()\n",
    "            mean_d = np.average(G[\"dist_km\"], weights=G[\"flow\"]) if total > 0 else np.nan\n",
    "            print(f\"  total trips={total:,.0f} | flow-weighted mean dist={mean_d:,.1f} km\")\n",
    "            # Optional: check symmetry stats (not required for doubly-constrained but useful)\n",
    "            # Build small pivot if feasible\n",
    "            n_hint = int(np.sqrt(len(G)))\n",
    "            if n_hint <= 300:  # avoid huge pivots\n",
    "                piv = G.pivot_table(index=\"oi\", columns=\"dj\", values=\"flow\", aggfunc=\"sum\").fillna(0.0)\n",
    "                asym = np.abs(piv.values - piv.values.T).mean()\n",
    "                print(f\"  mean asymmetry |F - F^T| = {asym:,.2f}\")\n",
    "    else:\n",
    "        print(\"[OD gravity] MISSING or EMPTY\")\n",
    "\n",
    "    # Agents\n",
    "    if exists_nonempty(ag):\n",
    "        A = pd.read_csv(ag)\n",
    "        need = {\"oi\",\"dj\",\"o_lon\",\"o_lat\",\"d_lon\",\"d_lat\"}\n",
    "        print(\"[OD agents] columns:\", ok(need.issubset(A.columns)), f\"N={len(A)}\")\n",
    "        # Spot-check coordinates in plausible bounds\n",
    "        for k in [\"o_lon\",\"d_lon\"]:\n",
    "            if k in A:\n",
    "                in_lon = A[k].between(-180, 180).mean()\n",
    "                print(f\"  {k} in [-180,180]: {in_lon:.2%}\")\n",
    "        for k in [\"o_lat\",\"d_lat\"]:\n",
    "            if k in A:\n",
    "                in_lat = A[k].between(-90, 90).mean()\n",
    "                print(f\"  {k} in [-90,90]: {in_lat:.2%}\")\n",
    "    else:\n",
    "        print(\"[OD agents] MISSING or EMPTY\")\n",
    "except Exception as e:\n",
    "    print(\"[OD] ERROR:\", e)\n",
    "\n",
    "# ------------------------------\n",
    "# 16) CROSS-FILE VALIDATION\n",
    "# ------------------------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CROSS-FILE VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # Lookup ↔ Admin2 Rank\n",
    "    if paths[\"lookup\"].exists() and paths[\"admin2_rank\"].exists():\n",
    "        lookup = pd.read_csv(paths[\"lookup\"])\n",
    "        admin2_rank = pd.read_csv(paths[\"admin2_rank\"])\n",
    "        lookup_codes = set(lookup.get(\"ADM2CD_c\", pd.Series(dtype=str)))\n",
    "        rank_codes = set(admin2_rank.get(\"ADM2CD_c\", pd.Series(dtype=str)))\n",
    "        codes_match = (lookup_codes == rank_codes)\n",
    "        print(\"[Lookup ↔ Admin2 Rank] ADM2CD_c match:\", ok(codes_match))\n",
    "        if not codes_match:\n",
    "            missing_in_rank = sorted(list(lookup_codes - rank_codes))[:10]\n",
    "            missing_in_lookup = sorted(list(rank_codes - lookup_codes))[:10]\n",
    "            if missing_in_rank:\n",
    "                print(f\"  Missing in rank (first 10): {missing_in_rank}\")\n",
    "            if missing_in_lookup:\n",
    "                print(f\"  Missing in lookup (first 10): {missing_in_lookup}\")\n",
    "\n",
    "    # Lookup ↔ Muni Indicators count\n",
    "    if paths[\"lookup\"].exists() and paths[\"muni\"].exists():\n",
    "        lookup = pd.read_csv(paths[\"lookup\"])\n",
    "        muni = pd.read_csv(paths[\"muni\"])\n",
    "        n_lookup = len(lookup)\n",
    "        n_muni_unique = len(muni[[\"ADM2CD_c\"]].drop_duplicates()) if \"ADM2CD_c\" in muni.columns else None\n",
    "        count_match = (n_muni_unique == n_lookup) if n_muni_unique is not None else False\n",
    "        print(\"[Lookup ↔ Muni Indicators] count match:\", ok(count_match),\n",
    "              f\"lookup={n_lookup} muni_unique={n_muni_unique}\")\n",
    "\n",
    "    # Check if admin2_rank matches the older rank file structure (intentional alias)\n",
    "    if admin2_rank is not None and exists_nonempty(paths[\"rank\"]):\n",
    "        old_rank = pd.read_csv(paths[\"rank\"])\n",
    "        common_cols = set(admin2_rank.columns) & set(old_rank.columns)\n",
    "        if len(common_cols) >= 5:\n",
    "            print(\"[Admin2 Rank ↔ Priority Rank] OK: same schema by design (legacy alias maintained).\")\n",
    "\n",
    "    # OD zones ↔ lookup (count consistency if both exist)\n",
    "    if paths[\"od_zone\"].exists() and paths[\"lookup\"].exists():\n",
    "        Z = pd.read_csv(paths[\"od_zone\"])\n",
    "        L = pd.read_csv(paths[\"lookup\"])\n",
    "        # Try to infer the ID column in Z\n",
    "        z_id = None\n",
    "        for cand in [\"ADM2CD_c\",\"adm2cd_c\",\"lab\",\"id\"]:\n",
    "            if cand in Z.columns:\n",
    "                z_id = cand\n",
    "                break\n",
    "        if z_id is not None:\n",
    "            n_match = len(set(Z[z_id])) == len(L)\n",
    "            print(\"[OD zones ↔ Lookup] zone count matches:\", ok(n_match), f\"zones={len(Z)} lookup={len(L)}\")\n",
    "        else:\n",
    "            print(\"[OD zones ↔ Lookup] SKIP (no recognizable zone id column)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"[Cross-validation] ERROR:\", e)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VALIDATION COMPLETE\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea00f220-c88c-4289-913a-4c28af265772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
